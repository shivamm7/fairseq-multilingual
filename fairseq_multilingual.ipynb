{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fairseq-multilingual.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "JJA5X-oHxNEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Files\n",
        "!pip3 install gdown -U\n",
        "\n",
        "!git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "!pip3 install indic_nlp_library\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!pip3 install -r indic_nlp_library/requirements.txt\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "\n",
        "! pip3 install https://github.com/rsennrich/subword-nmt/archive/master.zip\n",
        "\n",
        "\n",
        "!pip3 install subword_nmt\n",
        "!pip3 install sacrebleu\n",
        "!pip3 install ctranslate2\n",
        "!pip3 install mosestokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2AqBkqHcxPRi",
        "outputId": "050ec072-8440-44bc-9ded-f6f2cd4d6554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148097, done.\u001b[K\n",
            "remote: Counting objects: 100% (525/525), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 148097 (delta 323), reused 441 (delta 292), pack-reused 147572\u001b[K\n",
            "Receiving objects: 100% (148097/148097), 129.88 MiB | 17.07 MiB/s, done.\n",
            "Resolving deltas: 100% (114349/114349), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting indic_nlp_library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic_nlp_library) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic_nlp_library) (1.21.6)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic_nlp_library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic_nlp_library) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic_nlp_library) (1.15.0)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic_nlp_library) (1.8.6)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (0.17.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (0.7.12)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2.6.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (21.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2.10.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2.11.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (1.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (2022.5.18.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic_nlp_library) (1.1.5)\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.81 morfessor-2.0.6 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1325, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1325 (delta 91), reused 82 (delta 82), pack-reused 1218\u001b[K\n",
            "Receiving objects: 100% (1325/1325), 9.55 MiB | 15.87 MiB/s, done.\n",
            "Resolving deltas: 100% (701/701), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.7/dist-packages (from -r indic_nlp_library/requirements.txt (line 1)) (0.3.1)\n",
            "Requirement already satisfied: sphinx_rtd_theme in /usr/local/lib/python3.7/dist-packages (from -r indic_nlp_library/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.7/dist-packages (from -r indic_nlp_library/requirements.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r indic_nlp_library/requirements.txt (line 4)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r indic_nlp_library/requirements.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (1.8.6)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2.10.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (0.17.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (0.7.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (1.2.4)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2022.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (2022.5.18.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r indic_nlp_library/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->-r indic_nlp_library/requirements.txt (line 1)) (1.1.5)\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 16.42 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/rsennrich/subword-nmt/archive/master.zip\n",
            "  Downloading https://github.com/rsennrich/subword-nmt/archive/master.zip\n",
            "\u001b[K     - 133 kB 4.8 MB/s\n",
            "\u001b[?25hCollecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from subword-nmt==0.3.8) (4.64.0)\n",
            "Building wheels for collected packages: subword-nmt\n",
            "  Building wheel for subword-nmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subword-nmt: filename=subword_nmt-0.3.8-py3-none-any.whl size=27420 sha256=520879f229841d718eb6ef0f1a3bba0db2c8271ec2bf010a3759c61f5ba700c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-popj1o42/wheels/a8/3d/cb/91ddbecab4e8aa8a5bf2ebc531332bf2f6f91aedca09d0f4c8\n",
            "Successfully built subword-nmt\n",
            "Installing collected packages: mock, subword-nmt\n",
            "Successfully installed mock-4.0.3 subword-nmt-0.3.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: subword_nmt in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from subword_nmt) (4.64.0)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword_nmt) (4.0.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.4.0 sacrebleu-2.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ctranslate2\n",
            "  Downloading ctranslate2-2.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ctranslate2) (1.21.6)\n",
            "Collecting pyyaml<7,>=5.3\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyyaml, ctranslate2\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed ctranslate2-2.18.0 pyyaml-6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from mosestokenizer) (0.6.2)\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "Building wheels for collected packages: mosestokenizer, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49189 sha256=7047e9a31e661c61b7c838841421b85b0e45ed9ea4af3aac41996b7a7b57fc9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/35/f7/af1258779a0b890abc3c79481460c597cb1f3659d0603cfb9d\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3353 sha256=0a5a13ce39e388d3ca48969b060febf75db5fa5f17beafc91490135af7ee0a6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4f/33/54741ffe08e38ececb1d28068a153729b4fe820bafa0a0691f\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6161 sha256=dec033a22524ef1062474f388bfc960fc016c5183f877d41e3d0e22592686cfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/44/e9/914cf8fa71f0141f9314f862538d1218fcf2b94542a0fb7d35\n",
            "Successfully built mosestokenizer toolwrapper uctools\n",
            "Installing collected packages: uctools, toolwrapper, openfile, mosestokenizer\n",
            "Successfully installed mosestokenizer-1.2.1 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq\n",
        "!pip install ./\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CxKRaPvvAY8K",
        "outputId": "2ecd1d5b-089c-4b8b-a9a3-aea7ac628231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 31434, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 31434 (delta 23), reused 39 (delta 20), pack-reused 31381\u001b[K\n",
            "Receiving objects: 100% (31434/31434), 21.69 MiB | 15.73 MiB/s, done.\n",
            "Resolving deltas: 100% (23110/23110), done.\n",
            "/content/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/fairseq\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (2019.12.20)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (0.11.0+cu113)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (2.1.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (1.15.0)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (1.21.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e0884db) (0.29.30)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+e0884db) (5.7.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+e0884db) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+e0884db) (4.2.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+e0884db) (2.4.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+e0884db) (0.4.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+e0884db) (0.8.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+e0884db) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+e0884db) (3.8.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+e0884db-cp37-cp37m-linux_x86_64.whl size=15365049 sha256=a750daf1c65734d9175c8d4a88342c0d2f57bffe679d24a6c32d68519ae78d32\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h8t8i6j9/wheels/7c/35/80/edbd520a1a7e615df007002aeea9f6bf5f3c8f9243e072f6ce\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=726927c228067c959ab7cc41e188be72cd2043f52bb9284b78fb82ebbd214a3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.5.1 fairseq-1.0.0a0+e0884db hydra-core-1.0.7 omegaconf-2.0.6\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 43
        },
        "id": "iq1FPrXkEz25",
        "outputId": "3ec2a8af-72aa-40c8-c4c3-403d39156f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-27 11:04:10 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export PYTHONPATH=$PYTHONPATH:/content/indic_nlp_library\n",
        "export INDIC_RESOURCES_PATH=/content/indic_nlp_resources\n",
        "\n",
        "echo $INDIC_RESOURCES_PATH\n",
        "echo $PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8K-XUOwwxXTa",
        "outputId": "357f4053-1d78-4dac-d13b-c9e5dd98dd90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/indic_nlp_resources\n",
            "/env/python:/content/fairseq/:/content/indic_nlp_library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data"
      ],
      "metadata": {
        "id": "mxYXDklixmis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/en-mr.zip\n",
        "! unzip en-mr.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "7j0bnxLWxsaz",
        "outputId": "3888f61c-8942-417e-eed6-3c6979928fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-27 11:04:10--  https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/en-mr.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.118.128, 74.125.200.128, 74.125.68.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.118.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 769637789 (734M) [application/zip]\n",
            "Saving to: ‘en-mr.zip’\n",
            "\n",
            "en-mr.zip           100%[===================>] 733.98M  84.7MB/s    in 9.1s    \n",
            "\n",
            "2022-05-27 11:04:20 (81.1 MB/s) - ‘en-mr.zip’ saved [769637789/769637789]\n",
            "\n",
            "Archive:  en-mr.zip\n",
            "   creating: en-mr/\n",
            " extracting: en-mr/train.mr          \n",
            " extracting: en-mr/train.en          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/en-hi.zip\n",
        "! unzip en-hi.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "mEdwBJMvyCL1",
        "outputId": "2dbeb4d8-7e36-4e0c-ee1e-406aa6946532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-27 11:04:25--  https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/en-hi.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.128, 74.125.24.128, 142.251.10.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2853073724 (2.7G) [application/zip]\n",
            "Saving to: ‘en-hi.zip’\n",
            "\n",
            "en-hi.zip           100%[===================>]   2.66G  92.9MB/s    in 29s     \n",
            "\n",
            "2022-05-27 11:04:55 (92.5 MB/s) - ‘en-hi.zip’ saved [2853073724/2853073724]\n",
            "\n",
            "Archive:  en-hi.zip\n",
            "   creating: en-hi/\n",
            " extracting: en-hi/train.hi          \n",
            " extracting: en-hi/train.en          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowering amount of data. For full training use all data\n",
        "sourceValidLen = 2000\n",
        "targetValidLen = 2000\n",
        "\n",
        "sourceTestLen = 10000\n",
        "targetTestLen = 10000\n",
        "\n",
        "sourceTrainLen = 50000\n",
        "targetTrainLen = 50000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bXb8C_obyKAT",
        "outputId": "b3d65cd3-c728-4155-8b74-af52020e4f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir data/en-mr\n",
        "sourceData = open(\"en-mr/train.en\", 'r').readlines()\n",
        "targetData = open(\"en-mr/train.mr\", 'r').readlines()\n",
        "\n",
        "sourceValid = sourceData[0:sourceValidLen]\n",
        "targetValid = targetData[0:targetValidLen]\n",
        "\n",
        "sourceTest = sourceData[sourceValidLen:sourceValidLen + sourceTestLen]\n",
        "targetTest = targetData[targetValidLen:targetValidLen + targetTestLen]\n",
        "\n",
        "sourceTrain = sourceData[sourceValidLen + sourceTestLen:sourceValidLen + sourceTestLen + sourceTrainLen]\n",
        "targetTrain = targetData[targetValidLen + targetTestLen:targetValidLen + targetTestLen + targetTrainLen]\n",
        "\n",
        "sourceTestFile = open(\"data/en-mr/test.en\", \"w+\")\n",
        "for line in sourceTest:\n",
        "  sourceTestFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "sourceTestFile.close()\n",
        "\n",
        "targetTestFile = open(\"data/en-mr/test.mr\", \"w+\")\n",
        "for line in targetTest:\n",
        "  targetTestFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "targetTestFile.close()\n",
        "\n",
        "sourceValidFile = open(\"data/en-mr/valid.en\", \"w+\")\n",
        "for line in sourceValid:\n",
        "  sourceValidFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "sourceValidFile.close()\n",
        "\n",
        "targetValidFile = open(\"data/en-mr/valid.mr\", \"w+\")\n",
        "for line in targetValid:\n",
        "  targetValidFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "targetValidFile.close()\n",
        "\n",
        "sourceTrainFile = open(\"data/en-mr/train.en\", \"w+\")\n",
        "for line in sourceTrain:\n",
        "  sourceTrainFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "sourceTrainFile.close()\n",
        "\n",
        "targetTrainFile = open(\"data/en-mr/train.mr\", \"w+\")\n",
        "for line in targetTrain:\n",
        "  targetTrainFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "targetTrainFile.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8eJamfuWyKoL",
        "outputId": "0599ddca-7fdb-4acf-d7dc-408fc69648f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data/en-hi\n",
        "sourceData = open(\"en-hi/train.en\", 'r').readlines()\n",
        "targetData = open(\"en-hi/train.hi\", 'r').readlines()\n",
        "\n",
        "sourceValid = sourceData[0:sourceValidLen]\n",
        "targetValid = targetData[0:targetValidLen]\n",
        "\n",
        "sourceTest = sourceData[sourceValidLen:sourceValidLen + sourceTestLen]\n",
        "targetTest = targetData[targetValidLen:targetValidLen + targetTestLen]\n",
        "\n",
        "sourceTrain = sourceData[sourceValidLen + sourceTestLen:sourceValidLen + sourceTestLen + sourceTrainLen]\n",
        "targetTrain = targetData[targetValidLen + targetTestLen:targetValidLen + targetTestLen + targetTrainLen]\n",
        "\n",
        "sourceTestFile = open(\"data/en-hi/test.en\", \"w+\")\n",
        "for line in sourceTest:\n",
        "  sourceTestFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "sourceTestFile.close()\n",
        "\n",
        "targetTestFile = open(\"data/en-hi/test.hi\", \"w+\")\n",
        "for line in targetTest:\n",
        "  targetTestFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "targetTestFile.close()\n",
        "\n",
        "sourceValidFile = open(\"data/en-hi/valid.en\", \"w+\")\n",
        "for line in sourceValid:\n",
        "  sourceValidFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "sourceValidFile.close()\n",
        "\n",
        "targetValidFile = open(\"data/en-hi/valid.hi\", \"w+\")\n",
        "for line in targetValid:\n",
        "  targetValidFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "targetValidFile.close()\n",
        "\n",
        "sourceTrainFile = open(\"data/en-hi/train.en\", \"w+\")\n",
        "for line in sourceTrain:\n",
        "  sourceTrainFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "sourceTrainFile.close()\n",
        "\n",
        "targetTrainFile = open(\"data/en-hi/train.hi\", \"w+\")\n",
        "for line in targetTrain:\n",
        "  targetTrainFile.write(line.strip(\"\\n\") + \"\\n\")\n",
        "targetTrainFile.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QAQgGOrWyZYz",
        "outputId": "f07dcdfb-e3c5-4755-beef-059c2f1f3fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "MwtoVV7Jyjp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lowercase English"
      ],
      "metadata": {
        "id": "OqGSQcVEyyEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/mosesdecoder/scripts/tokenizer/lowercase.perl < data/en-mr/train.en > data/en-mr/train-low.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/lowercase.perl < data/en-mr/test.en > data/en-mr/test-low.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/lowercase.perl < data/en-mr/valid.en > data/en-mr/valid-low.en\n",
        "\n",
        "!/content/mosesdecoder/scripts/tokenizer/lowercase.perl < data/en-hi/train.en > data/en-hi/train-low.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/lowercase.perl < data/en-hi/test.en > data/en-hi/test-low.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/lowercase.perl < data/en-hi/valid.en > data/en-hi/valid-low.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EZH1x027ylUE",
        "outputId": "91059184-f723-414e-838e-7796318ec132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 data/en-mr/train.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "hlC5FsHby0jy",
        "outputId": "464cbcd7-2122-4c04-c955-1166ea2fd02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some are against it.\n",
            "[ Footnote] Capernaum was considered to be Jesus home city in the district of Galilee. Mark 2: 1.\n",
            "PM Narendra Modi and Amit Shah have already held rallies in the state.\n",
            "childrens education\n",
            "In many places there had been clashes.\n",
            "He had joined BJP before the Rajya Sabha polls.\n",
            "Uddhav Thackeray had visited Ayodhya before the Lok Sabha polls.\n",
            "Further arrests are expected in the future.\n",
            "Dont like it.\n",
            "So, lets say we just put one electrode of lithium here another electrode of something some counter electrode here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 data/en-mr/train-low.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "P3td5U9Jy33T",
        "outputId": "980eab51-10e9-4f22-ff70-eef19b0ae8ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "some are against it.\n",
            "[ footnote] capernaum was considered to be jesus home city in the district of galilee. mark 2: 1.\n",
            "pm narendra modi and amit shah have already held rallies in the state.\n",
            "childrens education\n",
            "in many places there had been clashes.\n",
            "he had joined bjp before the rajya sabha polls.\n",
            "uddhav thackeray had visited ayodhya before the lok sabha polls.\n",
            "further arrests are expected in the future.\n",
            "dont like it.\n",
            "so, lets say we just put one electrode of lithium here another electrode of something some counter electrode here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize English"
      ],
      "metadata": {
        "id": "xTAKfgPPy7Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/mosesdecoder/scripts/tokenizer/tokenizer.perl < data/en-mr/train-low.en > data/en-mr/train-tok.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/tokenizer.perl < data/en-mr/test-low.en > data/en-mr/test-tok.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/tokenizer.perl < data/en-mr/valid-low.en > data/en-mr/valid-tok.en\n",
        "\n",
        "!/content/mosesdecoder/scripts/tokenizer/tokenizer.perl < data/en-hi/train-low.en > data/en-hi/train-tok.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/tokenizer.perl < data/en-hi/test-low.en > data/en-hi/test-tok.en\n",
        "!/content/mosesdecoder/scripts/tokenizer/tokenizer.perl < data/en-hi/valid-low.en > data/en-hi/valid-tok.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "m0fC4H_Ay9Ur",
        "outputId": "db661538-3d2a-4c3b-dc7e-ab6566790977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 data/en-mr/train-tok.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "Px4R-SRfzJ8D",
        "outputId": "795b224c-816d-400e-e484-fbb638df1ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "some are against it .\n",
            "&#91; footnote &#93; capernaum was considered to be jesus home city in the district of galilee. mark 2 : 1 .\n",
            "pm narendra modi and amit shah have already held rallies in the state .\n",
            "childrens education\n",
            "in many places there had been clashes .\n",
            "he had joined bjp before the rajya sabha polls .\n",
            "uddhav thackeray had visited ayodhya before the lok sabha polls .\n",
            "further arrests are expected in the future .\n",
            "dont like it .\n",
            "so , lets say we just put one electrode of lithium here another electrode of something some counter electrode here .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize Indic"
      ],
      "metadata": {
        "id": "p6rdapaQzL2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 indic_nlp_library/indicnlp/normalize/indic_normalize.py data/en-mr/train.mr data/en-mr/train-norm.mr mr\n",
        "!python3 indic_nlp_library/indicnlp/normalize/indic_normalize.py data/en-mr/valid.mr data/en-mr/valid-norm.mr mr\n",
        "!python3 indic_nlp_library/indicnlp/normalize/indic_normalize.py data/en-mr/test.mr data/en-mr/test-norm.mr mr\n",
        "\n",
        "!python3 indic_nlp_library/indicnlp/normalize/indic_normalize.py data/en-hi/train.hi data/en-hi/train-norm.hi hi\n",
        "!python3 indic_nlp_library/indicnlp/normalize/indic_normalize.py data/en-hi/valid.hi data/en-hi/valid-norm.hi hi\n",
        "!python3 indic_nlp_library/indicnlp/normalize/indic_normalize.py data/en-hi/test.hi data/en-hi/test-norm.hi hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Z7sOMYdwzOry",
        "outputId": "483a55ad-69ba-40a2-fa76-029055aea29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 data/en-mr/train.mr "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "z-DnLP8UzeMv",
        "outputId": "29f5ec04-4fd9-41da-fb2a-ad962659c821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तर काही जण विरोधही करत आहेत.\n",
            "या माहितीचे परीक्षण करताना आपल्याला कळेल, की आजही येशूचा लोकांवर प्रभाव का पडत आहे. (w१० - E ०४ / ०१)\n",
            "नरेंद्र मोदी आणि अमित शहा यांनी देशात अराजक माजवले आहे.\n",
            "मुलाचे शिक्षण\n",
            "अनेक ठिकाणी पडझडीच्या घटना घडल्या.\n",
            "मात्र विधानसभा निवडणुकीआधी ते भाजपमध्ये गेले.\n",
            "लोकसभा निवडणुकीच्या आधी पक्षप्रमुख उद्धव ठाकरे यांनी अयोध्येचा दौरा केला होता.\n",
            "येत्या काळात आणखी काहींची धरपकड केली जाण्याची शक्यता आहे.\n",
            "अजिबात आवडला नाही.\n",
            "तर, आपण इथे लिथियमचे एक इलेक्ट्रोड (electrode) ठेवू या.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 data/en-mr/train-norm.mr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "9QIunRF-zf9S",
        "outputId": "7a9990a6-ccfb-45f4-fafd-483b6a8a0710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तर काही जण विरोधही करत आहेत.\n",
            "या माहितीचे परीक्षण करताना आपल्याला कळेल, की आजही येशूचा लोकांवर प्रभाव का पडत आहे. (w१० - E ०४ / ०१)\n",
            "नरेंद्र मोदी आणि अमित शहा यांनी देशात अराजक माजवले आहे.\n",
            "मुलाचे शिक्षण\n",
            "अनेक ठिकाणी पडझडीच्या घटना घडल्या.\n",
            "मात्र विधानसभा निवडणुकीआधी ते भाजपमध्ये गेले.\n",
            "लोकसभा निवडणुकीच्या आधी पक्षप्रमुख उद्धव ठाकरे यांनी अयोध्येचा दौरा केला होता.\n",
            "येत्या काळात आणखी काहींची धरपकड केली जाण्याची शक्यता आहे.\n",
            "अजिबात आवडला नाही.\n",
            "तर, आपण इथे लिथियमचे एक इलेक्ट्रोड (electrode) ठेवू या.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize Indic"
      ],
      "metadata": {
        "id": "JHnALLOlzkeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Uncomment if the part after `if __name__ == '__main__':` in the file `indic_nlp_library/indicnlp/tokenize/indic_tokenize.py`"
      ],
      "metadata": {
        "id": "J592MgEhT9yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 indic_nlp_library/indicnlp/tokenize/indic_tokenize.py data/en-mr/train-norm.mr data/en-mr/train-tok.mr mr\n",
        "!python3 indic_nlp_library/indicnlp/tokenize/indic_tokenize.py data/en-mr/valid-norm.mr data/en-mr/valid-tok.mr mr\n",
        "!python3 indic_nlp_library/indicnlp/tokenize/indic_tokenize.py data/en-mr/test-norm.mr data/en-mr/test-tok.mr mr\n",
        "\n",
        "!python3 indic_nlp_library/indicnlp/tokenize/indic_tokenize.py data/en-hi/train-norm.hi data/en-hi/train-tok.hi hi\n",
        "!python3 indic_nlp_library/indicnlp/tokenize/indic_tokenize.py data/en-hi/valid-norm.hi data/en-hi/valid-tok.hi hi\n",
        "!python3 indic_nlp_library/indicnlp/tokenize/indic_tokenize.py data/en-hi/test-norm.hi data/en-hi/test-tok.hi hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "syFCez_OzmTs",
        "outputId": "fa7bfb16-d007-4bcf-b798-e58a87bb6eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 data/en-mr/train-tok.mr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "OITTCBfk04l7",
        "outputId": "11c9db31-6d19-4113-8e40-ffbe6d24cc2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तर काही जण विरोधही करत आहेत . \n",
            "या माहितीचे परीक्षण करताना आपल्याला कळेल , की आजही येशूचा लोकांवर प्रभाव का पडत आहे . ( w१० - E ०४ / ०१ ) \n",
            "नरेंद्र मोदी आणि अमित शहा यांनी देशात अराजक माजवले आहे . \n",
            "मुलाचे शिक्षण\n",
            "अनेक ठिकाणी पडझडीच्या घटना घडल्या . \n",
            "मात्र विधानसभा निवडणुकीआधी ते भाजपमध्ये गेले . \n",
            "लोकसभा निवडणुकीच्या आधी पक्षप्रमुख उद्धव ठाकरे यांनी अयोध्येचा दौरा केला होता . \n",
            "येत्या काळात आणखी काहींची धरपकड केली जाण्याची शक्यता आहे . \n",
            "अजिबात आवडला नाही . \n",
            "तर , आपण इथे लिथियमचे एक इलेक्ट्रोड ( electrode ) ठेवू या . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Byte Pair Encoding (BPE)"
      ],
      "metadata": {
        "id": "Oqwp1vjq07SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir data/full\n",
        "! cat data/en-mr/train-tok.en data/en-mr/test-tok.en data/en-mr/valid-tok.en data/en-hi/train-tok.en data/en-hi/test-tok.en data/en-hi/valid-tok.en > data/full/full.en\n",
        "! cat data/en-mr/train-tok.mr data/en-mr/test-tok.mr data/en-mr/valid-tok.mr data/en-hi/train-tok.hi data/en-hi/test-tok.hi data/en-hi/valid-tok.hi > data/full/full.himr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Omb_QuOa1RgV",
        "outputId": "6d4584a8-3f0e-45b1-f9b2-e826d200f238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir codes\n",
        "!mkdir data/en-mr/bpe\n",
        "!mkdir data/en-hi/bpe\n",
        "\n",
        "!subword-nmt learn-bpe --num-workers 128 -s 16000 < data/full/full.en > codes/codes.en\n",
        "!subword-nmt learn-bpe --num-workers 128 -s 16000 < data/full/full.himr > codes/codes.himr\n",
        "\n",
        "!subword-nmt apply-bpe -c codes/codes.en < data/full/full.en > data/full/full-bpe.en\n",
        "!subword-nmt apply-bpe -c codes/codes.himr < data/full/full.himr > data/full/full-bpe.himr\n",
        "\n",
        "\n",
        "!subword-nmt apply-bpe -c codes/codes.en < data/en-mr/train-tok.en > data/en-mr/bpe/train-bpe.en\n",
        "!subword-nmt apply-bpe -c codes/codes.en < data/en-mr/test-tok.en > data/en-mr/bpe/test-bpe.en\n",
        "!subword-nmt apply-bpe -c codes/codes.en < data/en-mr/valid-tok.en > data/en-mr/bpe/valid-bpe.en\n",
        "\n",
        "!subword-nmt apply-bpe -c codes/codes.en < data/en-hi/train-tok.en > data/en-hi/bpe/train-bpe.en\n",
        "!subword-nmt apply-bpe -c codes/codes.en < data/en-hi/test-tok.en > data/en-hi/bpe/test-bpe.en\n",
        "!subword-nmt apply-bpe -c codes/codes.en < data/en-hi/valid-tok.en > data/en-hi/bpe/valid-bpe.en\n",
        "\n",
        "\n",
        "\n",
        "!subword-nmt apply-bpe -c codes/codes.himr < data/en-mr/train-tok.mr > data/en-mr/bpe/train-bpe.mr\n",
        "!subword-nmt apply-bpe -c codes/codes.himr < data/en-mr/test-tok.mr > data/en-mr/bpe/test-bpe.mr\n",
        "!subword-nmt apply-bpe -c codes/codes.himr < data/en-mr/valid-tok.mr > data/en-mr/bpe/valid-bpe.mr\n",
        "\n",
        "!subword-nmt apply-bpe -c codes/codes.himr < data/en-hi/train-tok.hi > data/en-hi/bpe/train-bpe.hi\n",
        "!subword-nmt apply-bpe -c codes/codes.himr < data/en-hi/test-tok.hi > data/en-hi/bpe/test-bpe.hi\n",
        "!subword-nmt apply-bpe -c codes/codes.himr < data/en-hi/valid-tok.hi > data/en-hi/bpe/valid-bpe.hi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "lpXtNLuI1I_T",
        "outputId": "5d9a953b-36a2-422c-dbeb-faaf3879cd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 16000/16000 [00:20<00:00, 794.20it/s]\n",
            "100% 16000/16000 [00:30<00:00, 522.22it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dictionary"
      ],
      "metadata": {
        "id": "Hc73ZJ_k6E2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir dict\n",
        "! python3 fairseq/fairseq_cli/preprocess.py --source-lang en --target-lang himr --trainpref data/full/full-bpe --destdir dict --workers 40 --dict-only"
      ],
      "metadata": {
        "id": "PQz4l8Kz6Ijc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0e4367ea-9f1d-45bc-e34b-3dfd9886a8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-27 11:07:55 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-27 11:07:56 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='dict', dict_only=True, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='himr', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='data/full/full-bpe', use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "lN2OwKr1_Cyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir pre\n",
        "! fairseq-preprocess --source-lang en --target-lang mr --srcdict dict/dict.en.txt --tgtdict dict/dict.himr.txt --trainpref data/en-mr/bpe/train-bpe --validpref data/en-mr/bpe/valid-bpe --testpref data/en-mr/bpe/test-bpe --destdir pre --workers 40\n",
        "! fairseq-preprocess --source-lang en --target-lang hi --srcdict dict/dict.en.txt --tgtdict dict/dict.himr.txt --trainpref data/en-hi/bpe/train-bpe --validpref data/en-hi/bpe/valid-bpe --testpref data/en-hi/bpe/test-bpe --destdir pre --workers 40\n"
      ],
      "metadata": {
        "id": "0QgQpFt2-6Hn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "outputId": "3a2583ea-af47-4b51-f16b-657ff16cbbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-27 11:08:14 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-27 11:08:14 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='pre', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict='dict/dict.en.txt', suppress_crashes=False, target_lang='mr', task='translation', tensorboard_logdir=None, testpref='data/en-mr/bpe/test-bpe', tgtdict='dict/dict.himr.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='data/en-mr/bpe/train-bpe', use_plasma_view=False, user_dir=None, validpref='data/en-mr/bpe/valid-bpe', wandb_project=None, workers=40)\n",
            "2022-05-27 11:08:14 | INFO | fairseq_cli.preprocess | [en] Dictionary: 15768 types\n",
            "2022-05-27 11:08:19 | INFO | fairseq_cli.preprocess | [en] data/en-mr/bpe/train-bpe.en: 50000 sents, 727367 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:19 | INFO | fairseq_cli.preprocess | [en] Dictionary: 15768 types\n",
            "2022-05-27 11:08:19 | INFO | fairseq_cli.preprocess | [en] data/en-mr/bpe/valid-bpe.en: 2000 sents, 29101 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:19 | INFO | fairseq_cli.preprocess | [en] Dictionary: 15768 types\n",
            "2022-05-27 11:08:21 | INFO | fairseq_cli.preprocess | [en] data/en-mr/bpe/test-bpe.en: 10000 sents, 144615 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:21 | INFO | fairseq_cli.preprocess | [mr] Dictionary: 16552 types\n",
            "2022-05-27 11:08:28 | INFO | fairseq_cli.preprocess | [mr] data/en-mr/bpe/train-bpe.mr: 50000 sents, 782071 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:28 | INFO | fairseq_cli.preprocess | [mr] Dictionary: 16552 types\n",
            "2022-05-27 11:08:29 | INFO | fairseq_cli.preprocess | [mr] data/en-mr/bpe/valid-bpe.mr: 2000 sents, 31226 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:29 | INFO | fairseq_cli.preprocess | [mr] Dictionary: 16552 types\n",
            "2022-05-27 11:08:31 | INFO | fairseq_cli.preprocess | [mr] data/en-mr/bpe/test-bpe.mr: 10000 sents, 156622 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:31 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to pre\n",
            "2022-05-27 11:08:32 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-27 11:08:32 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='pre', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict='dict/dict.en.txt', suppress_crashes=False, target_lang='hi', task='translation', tensorboard_logdir=None, testpref='data/en-hi/bpe/test-bpe', tgtdict='dict/dict.himr.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='data/en-hi/bpe/train-bpe', use_plasma_view=False, user_dir=None, validpref='data/en-hi/bpe/valid-bpe', wandb_project=None, workers=40)\n",
            "2022-05-27 11:08:32 | INFO | fairseq_cli.preprocess | [en] Dictionary: 15768 types\n",
            "2022-05-27 11:08:38 | INFO | fairseq_cli.preprocess | [en] data/en-hi/bpe/train-bpe.en: 50000 sents, 1065385 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:38 | INFO | fairseq_cli.preprocess | [en] Dictionary: 15768 types\n",
            "2022-05-27 11:08:39 | INFO | fairseq_cli.preprocess | [en] data/en-hi/bpe/valid-bpe.en: 2000 sents, 42563 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:39 | INFO | fairseq_cli.preprocess | [en] Dictionary: 15768 types\n",
            "2022-05-27 11:08:40 | INFO | fairseq_cli.preprocess | [en] data/en-hi/bpe/test-bpe.en: 10000 sents, 212400 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:40 | INFO | fairseq_cli.preprocess | [hi] Dictionary: 16552 types\n",
            "2022-05-27 11:08:49 | INFO | fairseq_cli.preprocess | [hi] data/en-hi/bpe/train-bpe.hi: 50000 sents, 1216241 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:49 | INFO | fairseq_cli.preprocess | [hi] Dictionary: 16552 types\n",
            "2022-05-27 11:08:50 | INFO | fairseq_cli.preprocess | [hi] data/en-hi/bpe/valid-bpe.hi: 2000 sents, 47846 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:50 | INFO | fairseq_cli.preprocess | [hi] Dictionary: 16552 types\n",
            "2022-05-27 11:08:53 | INFO | fairseq_cli.preprocess | [hi] data/en-hi/bpe/test-bpe.hi: 10000 sents, 243157 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-27 11:08:53 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to pre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "_gaG6Gkf_unf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "![multi.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "HACTivkxujqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir checkpoints\n",
        "! mkdir board\n",
        "\n",
        "! fairseq-train pre \\\n",
        "  --arch transformer_wmt_en_de \\\n",
        "  --task translation_multi_simple_epoch \\\n",
        "  --encoder-langtok src \\\n",
        "  --decoder-langtok \\\n",
        "  --lang-pairs en-hi,en-mr \\\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "  --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "  --lr-scheduler inverse_sqrt --lr 5e-04 --warmup-updates 4000 --max-update 400000 \\\n",
        "  --dropout 0.3 --weight-decay 0.0001 \\\n",
        "  --max-tokens 4096 --update-freq 8 \\\n",
        "  --save-interval 1 --save-interval-updates 5000 --keep-interval-updates 20 --no-epoch-checkpoints \\\n",
        "  --log-format simple --log-interval 100 \\\n",
        "  --fp16 --save-dir checkpoints --validate-interval-updates 1000 \\\n",
        "  --tensorboard-logdir board"
      ],
      "metadata": {
        "id": "YFnfmteT_v9M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ec239b9-84ac-4c18-ea15-face426d1e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n",
            "mkdir: cannot create directory ‘board’: File exists\n",
            "2022-05-27 11:11:27 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-27 11:11:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'board', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 20, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_wmt_en_de', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_wmt_en_de', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='pre', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=20, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='en-hi,en-mr', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=4096, max_tokens_valid=4096, max_update=400000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='checkpoints', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_dict=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_dict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir='board', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[8], update_ordered_indices_seed=False, upsample_primary=1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=1000, virtual_data_size=None, virtual_epoch_size=None, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='translation_multi_simple_epoch', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_wmt_en_de', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='pre', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=20, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, lang_dict=None, lang_pairs='en-hi,en-mr', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=400000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='checkpoints', save_interval=1, save_interval_updates=5000, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_dict=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_dict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir='board', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[8], update_ordered_indices_seed=False, upsample_primary=1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=1000, virtual_data_size=None, virtual_epoch_size=None, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-05-27 11:11:29 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
            "2022-05-27 11:11:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['en', 'hi', 'mr']\n",
            "2022-05-27 11:11:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 15771 types\n",
            "2022-05-27 11:11:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 16555 types\n",
            "2022-05-27 11:11:30 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(15771, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(16555, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=16555, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-05-27 11:11:30 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask\n",
            "2022-05-27 11:11:30 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-05-27 11:11:30 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-05-27 11:11:30 | INFO | fairseq_cli.train | num. shared model params: 69,165,568 (num. trained: 69,165,568)\n",
            "2022-05-27 11:11:30 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-05-27 11:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
            "2022-05-27 11:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=4890.66015625Mb; avail=8209.5Mb\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-hi': 1, 'main:en-mr': 1}\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-hi src_langtok: 15768; tgt_langtok: 16553\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: pre/valid.en-hi.en\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: pre/valid.en-hi.hi\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | pre valid en-hi 2000 examples\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-mr src_langtok: 15768; tgt_langtok: 16554\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: pre/valid.en-mr.en\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: pre/valid.en-mr.mr\n",
            "2022-05-27 11:11:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | pre valid en-mr 2000 examples\n",
            "2022-05-27 11:11:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-05-27 11:11:34 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-05-27 11:11:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-05-27 11:11:34 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-05-27 11:11:34 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-05-27 11:11:34 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
            "2022-05-27 11:11:34 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2022-05-27 11:11:34 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7612.8125Mb; avail=6785.8359375Mb\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:en-hi': 1, 'main:en-mr': 1}\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-hi src_langtok: 15768; tgt_langtok: 16553\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: pre/train.en-hi.en\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: pre/train.en-hi.hi\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | pre train en-hi 50000 examples\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-mr src_langtok: 15768; tgt_langtok: 16554\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: pre/train.en-mr.en\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.data_utils | loaded 50,000 examples from: pre/train.en-mr.mr\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | pre train en-mr 50000 examples\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:en-hi', 50000), ('main:en-mr', 50000)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:en-hi': 50000, 'main:en-mr': 50000}; raw total size: 100000\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:en-hi': 50000, 'main:en-mr': 50000}; resampled total size: 100000\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.009597\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7616.9296875Mb; avail=6784.0390625Mb\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.001581\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.014361\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7616.9296875Mb; avail=6784.16796875Mb\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000689\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7616.9296875Mb; avail=6784.16796875Mb\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.005862\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.021619\n",
            "2022-05-27 11:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7616.9296875Mb; avail=6784.16796875Mb\n",
            "2022-05-27 11:11:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 82\n",
            "2022-05-27 11:11:34 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-05-27 11:11:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2022-05-27 11:11:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "2022-05-27 11:11:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "2022-05-27 11:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fairseq\n",
        "! python setup.py build_ext --inplace\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cNc3lUcZKpvx",
        "outputId": "b1971517-0d49-49b1-cd7e-92e7657d2142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "  for (rule of document.styleSheets[0].cssRules){\n",
              "    if (rule.selectorText=='body') {\n",
              "      rule.style.fontSize = '20px'\n",
              "      break\n",
              "    }\n",
              "  }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fairseq\n",
            "running build_ext\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\n",
            "cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/fairseq\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/fairseq\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.data_utils_fast' extension\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/data\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairseq/data/data_utils_fast.cpp:712\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.7/fairseq/data\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.token_block_utils_fast' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairseq/data/token_block_utils_fast.cpp:713\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairseq/data/token_block_utils_fast.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KPyArrayObject* __pyx_f_7fairseq_4data_22token_block_utils_fast__get_slice_indices_fast(PyArrayObject*, PyObject*, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kfairseq/data/token_block_utils_fast.cpp:3417:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "       __pyx_t_4 = ((\u001b[01;35m\u001b[K__pyx_v_sz_idx < __pyx_t_10\u001b[m\u001b[K) != 0);\n",
            "                     \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kfairseq/data/token_block_utils_fast.cpp:3612:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "       __pyx_t_3 = ((\u001b[01;35m\u001b[K__pyx_v_sz_idx < __pyx_t_10\u001b[m\u001b[K) != 0);\n",
            "                     \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.libbase' extension\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib/libbase\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fairseq/clib/libbase/balanced_assignment.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libbase -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libbase.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'fairseq.libnat' extension\n",
            "creating build/temp.linux-x86_64-3.7/fairseq/clib/libnat\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[Kfairseq/clib/libnat/edit_dist.cpp:9:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/pybind11/detail/common.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid pybind11::pybind11_fail(const string&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/pybind11/detail/common.h:734:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kinline declaration of ‘\u001b[01m\u001b[Kvoid pybind11::pybind11_fail(const string&)\u001b[m\u001b[K’ follows declaration with attribute noinline [\u001b[01;35m\u001b[K-Wattributes\u001b[m\u001b[K]\n",
            " [[noreturn]] PYBIND11_NOINLINE inline void pybind11_fail(const std::string &reason\u001b[01;35m\u001b[K)\u001b[m\u001b[K { throw std::runtime_error(reason); }\n",
            "                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/pybind11/detail/common.h:733:44:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kprevious definition of ‘\u001b[01m\u001b[Kvoid pybind11::pybind11_fail(const char*)\u001b[m\u001b[K’ was here\n",
            " [[noreturn]] PYBIND11_NOINLINE inline void \u001b[01;36m\u001b[Kpybind11_fail\u001b[m\u001b[K(const char *reason) { throw std::runtime_error(reason); }\n",
            "                                            \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/pybind11/detail/common.h:734:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kinline declaration of ‘\u001b[01m\u001b[Kvoid pybind11::pybind11_fail(const string&)\u001b[m\u001b[K’ follows declaration with attribute noinline [\u001b[01;35m\u001b[K-Wattributes\u001b[m\u001b[K]\n",
            " [[noreturn]] PYBIND11_NOINLINE inline void pybind11_fail(const std::string &reason\u001b[01;35m\u001b[K)\u001b[m\u001b[K { throw std::runtime_error(reason); }\n",
            "                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.7/dist-packages/torch/include/pybind11/detail/common.h:733:44:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kprevious definition of ‘\u001b[01m\u001b[Kvoid pybind11::pybind11_fail(const char*)\u001b[m\u001b[K’ was here\n",
            " [[noreturn]] PYBIND11_NOINLINE inline void \u001b[01;36m\u001b[Kpybind11_fail\u001b[m\u001b[K(const char *reason) { throw std::runtime_error(reason); }\n",
            "                                            \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so\n",
            "building 'alignment_train_cpu_binding' extension\n",
            "creating build/temp.linux-x86_64-3.7/examples\n",
            "creating build/temp.linux-x86_64-3.7/examples/operators\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c examples/operators/alignment_train_cpu.cpp -o build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=alignment_train_cpu_binding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libbase.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so -> \n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! fairseq-generate pre \\\n",
        "    --path checkpoints/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 \\\n",
        "    --source-lang en --target-lang hi \\\n",
        "    --task translation_multi_simple_epoch \\\n",
        "    --encoder-langtok src \\\n",
        "    --decoder-langtok \\\n",
        "    --lang-pairs en-hi,en-mr"
      ],
      "metadata": {
        "id": "xkEo7kLM99ri"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}